{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106b19e3-7f0a-4897-884a-9918feb5a838",
   "metadata": {},
   "source": [
    "# Keras Seqeuntial Artifical Neural Network (Output: ReLU/Sigmoid Activation) for Market Trading - updated 12/03/2022 ver 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391fcf4-4ac4-4ebc-950c-4ce3e5e30a68",
   "metadata": {},
   "source": [
    "## <i> i.) Import Libraries and Dependencies </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15adc14-d08e-4a9e-b837-ae97fe9c8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e361ee-9616-43a8-ab6f-8901c8d736d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras ANN Modeling Libraries & Sklearn Preprocessing Modules\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "# Import Keras ANN Dropout & Regularizer Build Processing Modules\n",
    "# Import dropout\n",
    "from keras.layers import Dropout\n",
    "# Import regularizers\n",
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee710fb3-aecb-462e-9454-36a74d4ff285",
   "metadata": {},
   "source": [
    "## <i> ii.) Prepare the DataFrames </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df8eb1-fdd3-4dce-9266-8bd2dc2bcbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filename from dir and store as 'algoData_asset' for each individual asset file to use later in labelling/filenaming\n",
    "filename = Path(\"../Datasets/algoData_S&P500.csv\")\n",
    "\n",
    "display(os.path.basename(filename))\n",
    "display(os.path.dirname(filename))\n",
    "display(os.path.splitext(filename))\n",
    "display(os.path.splitext(os.path.basename(filename)))\n",
    "algoData_asset = os.path.splitext(os.path.basename(filename))\n",
    "algoData_asset = algoData_asset[0] \n",
    "algoData_asset = algoData_asset.split('_')\n",
    "algoData_asset = algoData_asset[1]\n",
    "print(algoData_asset)\n",
    "\n",
    "#print(os.path.splitext(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de0d9b0-262a-455c-bb96-680f97db5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm algoData_asset is type(str)\n",
    "type(algoData_asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef5a30-77f6-41a7-8927-b830756cdadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import .csv file as dataframe and shift 'ActualReturns' column forward .shift() \n",
    "#to re-align to neutral (as the 'Signal' column and 'ActualReturns' are both shifted backed (.shift(-1)) in the raw .csv file\n",
    "\n",
    "# Import filename & conver to dataframe\n",
    "df = pd.read_csv(filename, \n",
    "                index_col='Date', \n",
    "                parse_dates=True,\n",
    "                infer_datetime_format=True)\n",
    "display(df.head(3))\n",
    "display(df.tail(3))\n",
    "\n",
    "# Create empty dataframe for 'ActualReturns' column and shift(+1) to realign to neutral timeseries index position\n",
    "actual_returns_shift_df = pd.DataFrame()\n",
    "actual_returns_shift_df['Norm Actual Returns'] = df['ActualReturns'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603035a-68b1-44e8-8f63-2a8dcba2ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_returns_shift_df = actual_returns_shift_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced91c2f-334e-4738-9ca1-3a1efc573181",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(actual_returns_shift_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4592b1-2335-4483-a3a8-f47cf79a30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list for non-normalized columns to be dropped out if required\n",
    "%pprint\n",
    "col_names = list(df.columns.values)\n",
    "display(col_names)\n",
    "display(f\"Number of unique columns: {len(col_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a807789-dea0-46df-8c7d-c8af3984451b",
   "metadata": {},
   "source": [
    "## <i> ii.) Split Data into Training & Test Sets </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed57f388-1952-4d7d-8b87-1ccb2985eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X features dataframe\n",
    "# Drop non-normalized & pre-Zscore calculated column values\n",
    "# Drop all Bear & Bull signals for Elder Ray as they do not apply to forex strategy only\n",
    "# I.e. 'lowerBB_normal', 'middleBB_normal', 'upperBB_normal', '2stdBB_normal'\n",
    "\n",
    "must_drop_feat = ['Ticker', 'Date.1']\n",
    "\n",
    "OHLCV = ['Open', 'High', 'Low', 'Close', 'Volume'] \n",
    "\n",
    "standard_feat = ['EMAShort', 'EMALong', 'RSIline', 'MOMline', 'ROCline', 'SMAline', 'lowerBB', \\\n",
    "                 'middleBB', 'upperBB', '2stdBB', 'MACDline', 'MACDHistogram', 'MACDSignal']\n",
    "\n",
    "normal_feat = ['EMAShort_normal', 'EMALong_normal', 'RSIline_normal', 'MOMline_normal', 'ROCline_normal', \\\n",
    "               'SMAline_normal', 'lowerBB_normal', 'middleBB_normal', 'upperBB_normal', \\\n",
    "               '2stdBB_normal', 'MACDline_normal', 'MACDHistogram_normal', 'MACDSignal_normal']\n",
    "\n",
    "zscore_feat = ['EMAShort_zscore', 'EMALong_zscore', 'RSIline_zscore', 'MOMline_zscore', 'ROCline_zscore', \\\n",
    "               'SMAline_zscore', 'lowerBB_zscore', 'middleBB_zscore', 'upperBB_zscore', \\\n",
    "               '2stdBB_zscore', 'MACDline_zscore', 'MACDHistogram_zscore', 'MACDSignal_zscore']\n",
    "\n",
    "\n",
    "\n",
    "X = df.copy()\n",
    "X = X.drop(columns = zscore_feat)\n",
    "X = X.drop(columns = standard_feat)\n",
    "X = X.drop(columns = OHLCV)\n",
    "X = X.drop(columns = must_drop_feat)\n",
    "#X = X.drop(columns = normal_feat)\n",
    "X['ActualReturns'] = actual_returns_shift_df['Norm Actual Returns']\n",
    "X = X.dropna()\n",
    "y = X[['Signal']]\n",
    "X = X.drop(columns = ['Signal'])\n",
    "\n",
    "display(f\"Number of included columns in final X-features: {len(X.columns)}\")\n",
    "display(X)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd227bd-7c9d-4a2d-8c49-a22ca8baad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Datasets Using 145 Week Groupings\n",
    "# Select trading begin\n",
    "training_begin = X.index.min()\n",
    "print(f\"Training begin: {training_begin}.\")\n",
    "\n",
    "# Select trading end\n",
    "# Note: months = 145 for all assets except ETH-USD (months= 14)\n",
    "# Note: months = 145 for all assets except BTC-USD (months= 70)\n",
    "training_end = X.index.min() + DateOffset(months = 145)\n",
    "print(f\"Training end: {training_end}.\")\n",
    "\n",
    "X_train = X.loc[training_begin:training_end] \n",
    "y_train = y.loc[training_begin:training_end] \n",
    "\n",
    "X_test = X.loc[training_end:]\n",
    "y_test = y.loc[training_end:]\n",
    "\n",
    "# Create Dataframe just to hold 'ActualReturns' values for end of calculations\n",
    "X_ActualReturns = actual_returns_shift_df.loc[training_end:]\n",
    "                \n",
    "#Also save a second X_test_aligned_index for final dataframe\n",
    "X_test_aligned_index = X.loc[training_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a957c8-08e5-4124-bc75-7e17ed77f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output y_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d2e3f-8aee-4689-bc40-51a893507c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output y_test\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23b64b-24a1-4081-b303-2229a7bc033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert all sell signals (-1) to be compatible with the output sigmoid activation function\n",
    "\n",
    "def convert_neg(y_df):\n",
    "    for i in (range(len(y_df))):\n",
    "        if (y_df['Signal'][i] == -1):\n",
    "            y_df['Signal'][i] = 0\n",
    "        \n",
    "    return y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7442e-edff-4fc5-9aff-1535dd297bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.copy()\n",
    "y_train = convert_neg(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4594c125-b028-4627-a201-934bec93183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.copy()\n",
    "y_test = convert_neg(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ca861-5d2c-46ce-b836-b74fad0c8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-test output y_train after -1->0 conversion for sigmoid output compatiblity \n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3daa006-aacd-4011-9829-21131c07201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-test output y_test after -1->0 conversion for sigmoid output compatiblity \n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee1f22-a3f1-4795-9ac8-7543b92c4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Test Set Dimensions\n",
    "\n",
    "display(f\"X Dataframe shape: {X.shape}.\")\n",
    "display(f\"y Dataframe shape: {y.shape}.\")\n",
    "display(f\"X_train Dataframe shape: {X_train.shape}.\")\n",
    "display(f\"y_train shape: {y_train.shape}.\")\n",
    "display(f\"X_test Dataframe shape: {X_test.shape}.\")\n",
    "display(f\"y_test shape: {y_test.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbac3b9-1d3b-4412-85dc-241413bacab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train/y_test dataframe ['Signal'] columns values to array for Keras Sequential fitting \n",
    "\n",
    "y_train = y_train['Signal'].values\n",
    "display(y_train[0:3])\n",
    "display(y_train[-1])\n",
    "y_test = y_test['Signal'].values\n",
    "display(y_test[0:3])\n",
    "display(y_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb423c6-92cb-4e77-b382-43084af76203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize the y_train & y_test data into a trainable array\n",
    "# Reshape into vertical vectors for use in Sequential Model algorithm\n",
    "\n",
    "#y_train = y_train['Signal'].values.astype(\"int32\")\n",
    "#y_train = y_train.reshape(-1,1).astype(\"int32\")\n",
    "y_train = y_train.reshape(-1,1).astype(\"int32\")\n",
    "\n",
    "#y_test = y_test['Signal'].values.astype(\"int32\")\n",
    "y_test = y_test.reshape(-1,1).astype(\"int32\")\n",
    "\n",
    "\n",
    "display((f\"Number of final X_test features to run into ANN: {len(X_test.columns)}.\"))\n",
    "display(y_train[0:10])\n",
    "display(y_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fc965-a166-4582-a38d-1a0295701982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StandardScaler instance\n",
    "X_scaler = StandardScaler()\n",
    "#X_train_scaled = pd.DataFrame(X_scaler.fit_transform(X_train),columns =X_train.columns)\n",
    "#X_test_scaled = pd.DataFrame(X_scaler.fit_transform(X_test),columns =X_test.columns)\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "# Scale the data \n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Display X_train_scaled & X_test_scaled shapes\n",
    "display(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "display(f\"X_train_scaled shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7768970e-04dd-4676-8638-82c5a0dc092f",
   "metadata": {},
   "source": [
    "## <i> iii.) Constructing the ANN Model Using Sequential Model </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e987790-8a80-489b-941a-c2e107ac5e79",
   "metadata": {},
   "source": [
    "### <i> Defining the Input Parameters to the ANN: </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f7a44-6f43-4607-8b11-002d4782982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "display(f\"Number of input features (N_i): {number_input_features}.\")\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons = 1\n",
    "display(f\"Number of output features (N_o): {number_output_neurons}.\")\n",
    "\n",
    "# Calculating optimal hidden layer neurons to use per layer (first layer primary)\n",
    "# (Calculation code frame added by me 11/12/2022)\n",
    "\n",
    "# N_h: Number of hidden layer neurons\n",
    "N_h = 0\n",
    "\n",
    "# N_i: Number of input neurons\n",
    "N_i = number_input_features\n",
    "display(f\"N_i: {N_i}\")\n",
    "\n",
    "# N_o: Number of output neurons\n",
    "N_o = number_output_neurons\n",
    "display(f\"N_o: {N_o}\")\n",
    "\n",
    "# N_s: Number of samples in training data set\n",
    "N_s = len(X_train)\n",
    "display(f\"N_s: {N_s}\")\n",
    "\n",
    "# alpha_: Arbitrary scaler between (2-10)\n",
    "alpha_ = 6\n",
    "display(f\"alpha_: {alpha_}\")\n",
    "\n",
    "# Calculate the new value of N_h:\n",
    "print()\n",
    "N_h = int(round((N_s / (alpha_ * (N_i + N_o))),0))\n",
    "display(f\"Final ideal value of N_h (ideal number of hidden nodes in [minimal] first layer) should be: {N_h}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8148c-f992-4a65-9948-d5842b314ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "#hidden_nodes_layer1 = N_h\n",
    "\n",
    "#hidden_nodes_layer1 = N_h\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 8\n",
    "hidden_nodes_layer3 = 8\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "display(f\"Number of hidden node layers (layer 1): {hidden_nodes_layer1}.\")\n",
    "\n",
    "# Instantiate the Artificial Neural Network 'Sequential' Model\n",
    "\n",
    "nn = Sequential()\n",
    "\n",
    "# Add LSTM (Long-short term memory layer)\n",
    "\n",
    "#nn.add(LSTM(units=10, activation=\"relu\", input_shape=(X_train_scaled.shape[0], X_train_scaled.shape[1])))\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(Dense(units=(hidden_nodes_layer2), activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(Dense(units=(hidden_nodes_layer3), activation=\"relu\"))\n",
    "\n",
    "# Fourth layer becomes dropout layer\n",
    "#model.add(Dropout(.2,input_shape=(10,)))\n",
    "#nn.add(Dropout(.1,input_shape=(number_input_features,)))\n",
    "\n",
    "# Fifth layer to add regularizer layering to the model\n",
    "#nn.add(Dense(5, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "nn.add(Dense(units=(14), activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "\n",
    "# Output layer\n",
    "nn.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229c948-85a7-4069-9527-505fe28f2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential Model\n",
    "#nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "# 1 epoch = when all samples (images) go through an entire iteration\n",
    "# batch size = fraction of the entire sample X_train data to be sent through one epoch\n",
    "# [epoch/batch_size = iterations per epoch] \n",
    "# 1 epoch when all samples go through forward/backward iteration\n",
    "\n",
    "print(\"Fit model on X_train_scaled data\")\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=275, verbose=2, batch_size=256, shuffle=False)\n",
    "\n",
    "# Plot the Output of the fit_model history\n",
    "fig, ax = plt.subplots(1) # Creates figure fig and add an axes, ax.\n",
    "fig2, ax2 = plt.subplots(1) # Another figure\n",
    "\n",
    "ax.set_title(f' Artificial Neural Network ((ReLU/Sigmoid Output Act.) - {algoData_asset} Accuracy Plot')\n",
    "ax.plot(fit_model.history['accuracy'], label='train')\n",
    "fig.savefig(f'../Datasets/algoData_ANN_model_acc_loss/{algoData_asset}_ann_accuracy_.png')\n",
    "\n",
    "\n",
    "ax2.set_title(f' Artificial Neural Network (ReLU/Sigmoid Output Act.) - {algoData_asset} Loss Plot')\n",
    "ax2.plot(fit_model.history['loss'], label='train')\n",
    "fig2.savefig(f'../Datasets/algoData_ANN_model_acc_loss/{algoData_asset}_ann_loss_.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6468797-daf9-4bf1-91f7-8618833031a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using testing data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display evaluation results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5f2cd-1feb-46ba-a7a9-791239103489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on X_test, y_test datasets\n",
    "\n",
    "print(\"Evaluate model on X_test_scaled data\")\n",
    "#predictions = nn.predict(X_test_scaled).astype(\"int32\")\n",
    "predictions = (nn.predict(X_test_scaled) > 0.51).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e6ce5-d066-45a7-8dcf-d28e28c93ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(predictions[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e1ae1-8726-4cc9-bd99-95dd23c2bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4123bd-bf6f-405f-9346-6c328d9c493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"predictions\": predictions.flatten(), \"actual\": y_test.flatten()})\n",
    "#results = pd.DataFrame({\"predictions\": predictions.ravel(), \"actual\": y_test}, index=[0])\n",
    "display(results.value_counts())\n",
    "display(results.head(10))\n",
    "display(results.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6737c9a1-0d4d-4961-b3cf-990d9e6f70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert all -1 to 0\n",
    "\n",
    "def convert_back_to_neg(y_df):\n",
    "    for i in (range(len(y_df))):\n",
    "        if (y_df['predictions'][i] == 0):\n",
    "            y_df['predictions'][i] = -1\n",
    "        \n",
    "    return y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12533058-ca8d-4a78-afea-95ade77abc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.copy()\n",
    "results = convert_back_to_neg(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf64f5-092a-4486-936b-4f0f45d28a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ddde32-cc8f-4dd2-b3e8-5f3140c070f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c2c58-25bf-453a-be40-f0721cf4f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original Actual Returns column from the original imported dataset\n",
    "\n",
    "df_final = df[['ActualReturns']]\n",
    "df_final = df_final.loc[training_end:]\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c3f4e-56c4-45b5-93c4-59109a3b7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to compile cumulative returns for Artificial Neural Network Predicted y_test values\n",
    "\n",
    "def annReturns(results, X_test):\n",
    "\n",
    "    # Make emtpy dataframe to store actual returns, predictions & cumulative returns\n",
    "    \n",
    "    # Inititialize first return 0 so cumulative ret begins at base 1\n",
    "    #annData['Actual Returns'][0] = 0\n",
    "    \n",
    "    annData = pd.DataFrame()\n",
    "    annData.index = X_test.index\n",
    "    results.index = X_test.index\n",
    "    annData['Prediction_Signal'] = results['predictions'] \n",
    "    annData['Actual_Returns'] = df_final['ActualReturns'].values\n",
    "    annData['annStrategyReturns'] = annData['Actual_Returns'] * annData['Prediction_Signal'] \n",
    "    annData['cumActual_Returns'] = (1 + annData['Actual_Returns']).cumprod()\n",
    "    annData['cumANNStrategyReturns'] = (1 + annData['annStrategyReturns']).cumprod()\n",
    "    \n",
    "    returns = annData\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b0933-fbb2-40db-8b40-46f95239bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run annReturns Function and display sample final cumulative returns dataframe\n",
    "\n",
    "returns_df = annReturns(results, X_test)\n",
    "display(returns_df.head(5))\n",
    "display(returns_df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d809b-8a1e-4b10-85d9-72dd0009942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results to External .csv File\n",
    "# Save Finalized Output 'returns_df' dataframe\n",
    "\n",
    "returns_df.to_csv(f'../Datasets/algoData_ANN_results/{algoData_asset}_ANN_results.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
